# Практическая работа №10: Параллельные и распределенные вычислительные системы

Данный проект посвящен исследованию различных технологий параллельного программирования: **OpenMP** (многопоточность CPU), **CUDA** (вычисления на GPU) и **MPI** (распределенные системы). Все задания выполнены и протестированы в среде **Google Colab**.

## Настройка окружения (Google Colab)

Для корректной работы всех заданий в Colab необходимо:

1. Включить GPU: **Изменить** -> **Настройки блокнота** -> **T4 GPU**.
2. Выполнить установку необходимых библиотек в первой ячейке:
```bash
!apt-get install -y mpi-default-dev mpich
!nvcc --version

```



---

## Описание заданий и запуск

### Задание 1. Анализ производительности CPU (OpenMP)

**Цель:** Оценка ускорения программы и анализ согласно закону Амдала.

* **Файл:** `p10_task1.cpp`
* **Реализация:** Вычисление среднего и дисперсии для массива из  элементов.
* **Команда запуска:**
```bash
!g++ -fopenmp p10_task1.cpp -o p10_task1 && ./p10_task1

```


* **Вывод:** Программа вычисляет долю параллельной части (). В ходе тестов , что подтверждает высокую эффективность распараллеливания циклов.

### Задание 2. Оптимизация памяти на GPU (CUDA)

**Цель:** Сравнение различных паттернов доступа к глобальной и разделяемой памяти.

* **Файл:** `p10_task2.cu`
* **Реализация:** Реализованы три ядра: коалесцированное, со смещением (stride) и с использованием `__shared__` памяти.
* **Команда запуска:**
```bash
!nvcc p10_task2.cu -o p10_task2 && ./p10_task2

```


* **Вывод:** Неэффективный (strided) доступ замедляет выполнение в разы. Использование Shared Memory позволяет минимизировать обращения к медленной глобальной памяти.

### Задание 3. Профилирование гибридного приложения (CPU + GPU)

**Цель:** Использование асинхронности для оптимизации взаимодействия Host-Device.

* **Файл:** `p10_task3.cu`
* **Реализация:** Применение `cudaStream_t` и `cudaMemcpyAsync` для одновременной передачи данных и вычислений.
* **Команда запуска:**
```bash
!nvcc p10_task3.cu -o task3 && ./task3

```


* **Вывод:** Асинхронные потоки позволяют "скрыть" задержки копирования данных за полезными вычислениями GPU.

### Задание 4. Масштабируемость распределенной программы (MPI)

**Цель:** Оценка Strong Scaling (сильного масштабирования) на различном числе процессов.

* **Файл:** `p10_task4.cpp`
* **Реализация:** Использование `MPI_Reduce` для агрегации данных.
* **Команда запуска (через Python-скрипт для чистого вывода):**
```python
# Запустить Python-ячейку с subprocess для последовательных тестов -np 1, 2, 4

```


* **Вывод:** На 2-х процессах достигнуто значительное ускорение. На 4-х процессах (в условиях 2-ядерной системы Colab) наблюдается стагнация времени из-за накладных расходов на коммуникацию.

---

##  Итоговые выводы

1. **OpenMP:** Эффективен для задач с интенсивными вычислениями на общих данных. Ограничен законом Амдала и последовательными участками (I/O, инициализация).
2. **CUDA:** Производительность GPU критически зависит от архитектуры доступа к памяти. Коалесценция — ключевой фактор оптимизации.
3. **Hybrid:** Комбинирование CPU и GPU требует балансировки и минимизации простоев через асинхронные стримы.
4. **MPI:** Позволяет масштабировать задачи на множество узлов, но требует учета накладных расходов на сетевые взаимодействия и синхронизацию.




---

## Автор проекта  
Студент: Aruzhan Bissimbayeva 

Группа: ADA-2403M

## Преподаватель  
Проверяющий: Sadvakassova Kuralay
